{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from imutils.video import VideoStream\n",
    "\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate the face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# detect the face rectangle \n",
    "def detect(img, cascade = face_cascade , minimumFeatureSize=(20, 20)):\n",
    "    if cascade.empty():\n",
    "        raise (Exception('There was a problem loading your Haar Cascade xml file.'))\n",
    "    rects = cascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=1, minSize=minimumFeatureSize)\n",
    " \n",
    "    # if it doesn’t return rectangle, return array with zero length\n",
    "    if len(rects) == 0:\n",
    "        return []\n",
    "    # detect mutliscale returns (x,y,w,h)\n",
    "    # x,y are the coordinates of the lower left corner\n",
    "    # w, h are the width and height of the rectangle\n",
    "    # convert last coord from (width,height) to (maxX, maxY)\n",
    "    rects[:, 2:] += rects[:, :2]\n",
    "    return rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(img, face_detector):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate the eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector\n",
    "predictor = dlib.shape_predictor('facial-landmarks/shape_predictor_68_face_landmarks.dat')\n",
    "def crop_eyes(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # detect the face at grayscale image\n",
    "    rect = detect(gray, minimumFeatureSize=(100, 100))\n",
    "    \n",
    "    # if the face detector doesn’t detect face\n",
    "    # return None, else if detects more than one faces\n",
    "    # keep the bigger and if it is only one keep one dim\n",
    "    if len(rect) == 0:\n",
    "        return None\n",
    "    elif len(rect) > 1:\n",
    "        face = rect[0]\n",
    "    elif len(rect) == 1:\n",
    "        [face] = rect\n",
    "        \n",
    "    # keep the face region from the whole frame\n",
    "    face_rect = dlib.rectangle(left = int(face[0]), top = int(face[1]),\n",
    "                               right = int(face[2]), bottom = int(face[3]))\n",
    "    \n",
    "    # determine the facial landmarks for the face region\n",
    "    shape = predictor(gray, face_rect)\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "            \n",
    "    # grab the indexes of the facial landmarks for the left and right eye, respectively\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS['right_eye']\n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS['left_eye']\n",
    "    \n",
    "    # extract the left and right eye coordinates\n",
    "    leftEye = shape[lStart:lEnd]\n",
    "    rightEye = shape[rStart:rEnd]\n",
    "    \n",
    "    # compute the convex hull for the left and right eye, then\n",
    "    # visualize each of the eyes\n",
    "    left_eye_hull = cv2.convexHull(leftEye)\n",
    "    right_eye_hull = cv2.convexHull(rightEye)\n",
    "    cv2.drawContours(frame, [left_eye_hull], -1, (0, 255, 0), 1)\n",
    "    cv2.drawContours(frame, [right_eye_hull], -1, (0, 255, 0), 1)\n",
    "        \n",
    "    # keep the upper and the lower limit of the eye and compute the height\n",
    "    l_uppery = min(leftEye[1:3,1])\n",
    "    l_lowy = max(leftEye[4:,1])\n",
    "    l_dify = abs(l_uppery - l_lowy)\n",
    "    \n",
    "    # compute the width of the eye\n",
    "    lw = (leftEye[3][0] - leftEye[0][0])\n",
    "    \n",
    "    # take a snapshot of the eye with 100x100 box to be reshaped later\n",
    "    minxl = (leftEye[0][0] - ((100-lw)/2))\n",
    "    maxxl = (leftEye[3][0] + ((100-lw)/2))\n",
    "    minyl = (l_uppery - ((100-l_dify)/2))\n",
    "    maxyl = (l_lowy + ((100-l_dify)/2))\n",
    "    \n",
    "    # crop the eye rectangle from the frame\n",
    "    left_eye_rect = np.rint([minxl, minyl, maxxl, maxyl])\n",
    "    left_eye_rect = left_eye_rect.astype(int)\n",
    "    left_eye_image = gray[(left_eye_rect[1]):left_eye_rect[3], (left_eye_rect[0]):left_eye_rect[2]]\n",
    "    \n",
    "    # same process for right eye\n",
    "    r_uppery = min(rightEye[1:3,1])\n",
    "    r_lowy = max(rightEye[4:,1])\n",
    "    r_dify = abs(r_uppery - r_lowy)\n",
    "    rw = (rightEye[3][0] - rightEye[0][0])\n",
    "    minxr = (rightEye[0][0]-((100-rw)/2))\n",
    "    maxxr = (rightEye[3][0] + ((100-rw)/2))\n",
    "    minyr = (r_uppery - ((100-r_dify)/2))\n",
    "    maxyr = (r_lowy + ((100-r_dify)/2))\n",
    "    right_eye_rect = np.rint([minxr, minyr, maxxr, maxyr])\n",
    "    right_eye_rect = right_eye_rect.astype(int)\n",
    "    right_eye_image = gray[right_eye_rect[1]:right_eye_rect[3], right_eye_rect[0]:right_eye_rect[2]]\n",
    "    \n",
    "    # if it doesn’t detect left or right eye return None\n",
    "    if 0 in left_eye_image.shape or 0 in right_eye_image.shape:\n",
    "        return None\n",
    "    \n",
    "    # resize for the conv net\n",
    "    left_eye_image = cv2.resize(left_eye_image, (24, 24))\n",
    "    right_eye_image = cv2.resize(right_eye_image, (24, 24))\n",
    "    right_eye_image = cv2.flip(right_eye_image, 1)\n",
    "    # return left and right eye\n",
    "    return left_eye_image, right_eye_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_preprocess(img):\n",
    "    img = img.astype('float32')\n",
    "    # scale between 0 and 1\n",
    "    img /= 255\n",
    "    # keras needs (row, width, height, channel)\n",
    "    # add two dimensions for number of images (row) and channel\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, weight_path):\n",
    "    json_file = open(model_path, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    # load model architecture\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(weight_path)\n",
    "    # compile the loaded model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sound_alarm(path):\n",
    "    # play an alarm sound\n",
    "    playsound.playsound(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    counter = 0\n",
    "    alarm_on = False\n",
    "    max_frame = 20\n",
    "    \n",
    "    # open the camera,load the cnn model\n",
    "    vs = VideoStream(src=0).start()\n",
    "    model = load_model('model.json', 'weights.h5')\n",
    "    time.sleep(2.0)\n",
    "    \n",
    "    while True:\n",
    "        frame = vs.read()\n",
    "        # detect eyes\n",
    "        eyes = crop_eyes(frame)\n",
    "        if eyes is None:\n",
    "            continue\n",
    "        else:\n",
    "            left_eye, right_eye = eyes  \n",
    "        \n",
    "        # predict if the eye is open or closed\n",
    "        state_left = model.predict_classes(cnn_preprocess(left_eye))[0][0]\n",
    "        state_right = model.predict_classes(cnn_preprocess(right_eye))[0][0]\n",
    "        \n",
    "        if state_left == 0 and state_right == 0:\n",
    "            counter += 1\n",
    "            cv2.putText(frame, \"Closed\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, \"For: {:.2f}\".format(counter), (300, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "            # sound alarm if closed too long \n",
    "            if counter >= max_frame:\n",
    "                # if the alarm is not on, turn it on\n",
    "                if not alarm_on:\n",
    "                    alarm_on = True\n",
    "                    t = Thread(target=sound_alarm,\n",
    "                                args=('siren.wav',))\n",
    "                    t.deamon = True\n",
    "                    t.start()\n",
    "\n",
    "                # draw an alarm on the frame\n",
    "                cv2.putText(frame, \"YOU'RE GOING TO CRASH!\", (100, 300),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "        else:\n",
    "            counter = 0\n",
    "            alarm_on = False\n",
    "            cv2.putText(frame, \"Opened\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "        # show the frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    vs.stop()       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/site-packages/keras/utils/conv_utils.py:82: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.copy(kernel[slices])\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-10befc4b31cf>\", line 3, in sound_alarm\n",
      "    playsound.playsound(path)\n",
      "NameError: name 'playsound' is not defined\n",
      "\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-10befc4b31cf>\", line 3, in sound_alarm\n",
      "    playsound.playsound(path)\n",
      "NameError: name 'playsound' is not defined\n",
      "\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-10befc4b31cf>\", line 3, in sound_alarm\n",
      "    playsound.playsound(path)\n",
      "NameError: name 'playsound' is not defined\n",
      "\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-10befc4b31cf>\", line 3, in sound_alarm\n",
      "    playsound.playsound(path)\n",
      "NameError: name 'playsound' is not defined\n",
      "\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-10befc4b31cf>\", line 3, in sound_alarm\n",
      "    playsound.playsound(path)\n",
      "NameError: name 'playsound' is not defined\n",
      "\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-10befc4b31cf>\", line 3, in sound_alarm\n",
      "    playsound.playsound(path)\n",
      "NameError: name 'playsound' is not defined\n",
      "\n",
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/brendonhapp/anaconda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-10befc4b31cf>\", line 3, in sound_alarm\n",
      "    playsound.playsound(path)\n",
      "NameError: name 'playsound' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
